This is a fork of "[SpinQuant: LLM Quantization with Learned Rotations](https://github.com/facebookresearch/SpinQuant)" repository that is intended to be integrated to CEVA's LiteML quantization tool.

# Installation
Create conda environment with python 3.9

```
conda create -n SpinQuant python=3.9
```
   
Install pytorch 2.2 with cuda 11.8
```
pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0
```
Follow the installation instructions in (https://github.com/facebookresearch/SpinQuant), summarized below:

```
git clone https://github.com/facebookresearch/SpinQuant.git
cd SpinQuant
pip install -r requirement.txt
git clone https://github.com/Dao-AILab/fast-hadamard-transform.git
cd fast-hadamard-transform
pip install .
```

Important note: before installing fast-hadamard-transform package, change os.rename(wheel_filename, wheel_path to shutil.move(wheel_filename, wheel_path) in run method of CachedWheelsCommand class in setup.py file of fast-hadamard-transform package.

# Run
## Applying rotations
The following command applies SpinQuant algorithm using GPTQ and weight clipping for a W4A8 configuration with a group size of 128 for the weights and activations.
It evaluates the perplexity on WikiText2 dataset and saves the model with the modified weights and scales in the path provided by save_qmodel_path argument.

```
python ptq.py --input_model meta-llama/Llama-2-7b-hf --do_train False --do_eval True --per_device_eval_batch_size 4 --model_max_length 2048 --fp16 True --bf16 False --save_safetensors False --w_bits 4 --a_bits 8 --k_bits 8 --v_bits 8 --k_groupsize 128 --v_groupsize 128 --w_groupsize 128 --a_groupsize 128 --rotate --w_clip --save_qmodel_path "./saved_models/spinquant_gptq_group128.pth.pth"
```

## Exporting the state dict
This script is used to convert the state dict file from the previous stage (Applying rotations) to a format supported by LiteML.

```
python liteml_state_dict.py --spinquant_path saved_models/spinquant_gptq_group128.pth --liteml_path saved_models/liteml_spinquant_gptq_group128_true_quant.pth --true_quant --group_size 128
```

### Possible arguments:
* **spinquant_path** - path to SpinQuant's state dict file that was generated by the ptq.py script.
* **liteml_path** - path to save the exported state dict file in LiteML format
* **true_quant** argument should be used to run this model in LiteML in "True Quant" mode, meaning that in each quantized layer, the weights and activations are first quantized, then multiplied and finally dequantized. If --true_quant is not used, during inference in LiteML the weights and activations are being quantized and dequantized before they are multiplied.
* **group_size** - group size used for activations and weights in group quantization

